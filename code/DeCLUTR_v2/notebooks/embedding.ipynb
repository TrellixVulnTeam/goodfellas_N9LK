{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "embedding.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1r5eABrxC3z",
        "colab_type": "text"
      },
      "source": [
        "# Embedding text with an existing model\n",
        "\n",
        "This notebook will walk you through embedding some text with a pretrained model using [DeCLUTR](https://github.com/JohnGiorgi/DeCLUTR). You can embed text in one of three ways:\n",
        "\n",
        "1. __As a library__: import and initialize an object from this repo, which can be used to embed sentences/paragraphs.\n",
        "2. __ü§ó Transformers__: load our pretrained model with the [ü§ó Transformers library](https://github.com/huggingface/transformers).\n",
        "3. __Bulk embed__: embed all text in a given text file with a simple command-line interface.\n",
        "\n",
        "Each approach has advantages and disadvantages:\n",
        "\n",
        "1. __As a library__: This is the easiest way to add DeCLUTR to an existing pipeline, but requires that you install our package.\n",
        "2. __ü§ó Transformers__: This only requires you to install the [ü§ó Transformers library](https://github.com/huggingface/transformers), but requires more boilerplate code.\n",
        "3. __Bulk embed__: This most suitable if you want to embed large quantities of text \"offline\" (e.g. not on-the-fly within an existing pipeline)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObkQs5cixC30",
        "colab_type": "text"
      },
      "source": [
        "## üîß Install the prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNQLlqrtOv5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/JohnGiorgi/DeCLUTR.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV_jB7IZxC4d",
        "colab_type": "text"
      },
      "source": [
        "Finally, let's check to see if we have a GPU available, which we can use to dramatically speed up the embedding of text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5jfZhdRxC4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    cuda_device = torch.cuda.current_device()\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    cuda_device = -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSVlxjYexC4k",
        "colab_type": "text"
      },
      "source": [
        "## 1Ô∏è‚É£ As a library\n",
        "\n",
        "To use the model as a library, import `Encoder` and pass it some text (it accepts both strings and lists of strings)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8pG_xeZxC4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from declutr import Encoder\n",
        "\n",
        "# This can be a path on disk to a model you have trained yourself OR\n",
        "# the name of one of our pretrained models.\n",
        "pretrained_model_or_path = \"declutr-small\"\n",
        "\n",
        "text = [\n",
        "    \"A smiling costumed woman is holding an umbrella.\",\n",
        "    \"A happy woman in a fairy costume holds an umbrella.\",\n",
        "    \"A soccer game with multiple males playing.\",\n",
        "    \"Some men are playing a sport.\",\n",
        "]\n",
        "\n",
        "encoder = Encoder(pretrained_model_or_path, cuda_device=cuda_device)\n",
        "embeddings = encoder(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjg7fRX4xC4s",
        "colab_type": "text"
      },
      "source": [
        "These embeddings can then be used, for example, to compute the semantic similarity between some number of sentences or paragraphs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvZ3ciiqVXBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Compute a semantic similarity via the cosine distance\n",
        "semantic_sim = 1 - cosine(embeddings[0], embeddings[1])\n",
        "print(semantic_sim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-mdfiUzVte-",
        "colab_type": "text"
      },
      "source": [
        "Mainly for fun, the following cells visualize the semantic similarity with a heatmap!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsE487dgxC4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_heatmap(text: List[str], embeddings: np.ndarray) -> None:\n",
        "    embeddings = torch.as_tensor(embeddings)\n",
        "    cosine = torch.nn.CosineSimilarity(-1)\n",
        "    similarity_matrix = []\n",
        "    for _, embedding in enumerate(embeddings):\n",
        "        similarity_vector = cosine(embedding, embeddings)\n",
        "        similarity_vector = similarity_vector.numpy()\n",
        "        similarity_matrix.append(similarity_vector)\n",
        "    df = pd.DataFrame(similarity_matrix)\n",
        "    df.columns = df.index = text\n",
        "    sns.heatmap(df, cmap=\"YlOrRd\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSc4u6XbUgPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_heatmap(text, embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAEXMEQ2xC41",
        "colab_type": "text"
      },
      "source": [
        "See the list of available `PRETRAINED_MODELS` in [declutr/encoder.py](https://github.com/JohnGiorgi/DeCLUTR/blob/master/declutr/encoder.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2imLEDWDxC42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from declutr.encoder import PRETRAINED_MODELS ; print(list(PRETRAINED_MODELS.keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9NudDYNxC46",
        "colab_type": "text"
      },
      "source": [
        "## 2Ô∏è‚É£ ü§ó Transformers\n",
        "\n",
        "Our pretrained models are also hosted with ü§ó Transformers, so they can be used like any other model in that library. Here is a simple example using [DeCLUTR-small](https://huggingface.co/johngiorgi/declutr-small):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a4Ymv39xC47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# Load the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"johngiorgi/declutr-small\")\n",
        "model = AutoModel.from_pretrained(\"johngiorgi/declutr-small\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Prepare some text to embed\n",
        "text = [\n",
        "    \"A smiling costumed woman is holding an umbrella.\",\n",
        "    \"A happy woman in a fairy costume holds an umbrella.\",\n",
        "    \"A soccer game with multiple males playing.\",\n",
        "    \"Some men are playing a sport.\",\n",
        "]\n",
        "inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "# Put the tensors on the GPU, if available\n",
        "for name, tensor in inputs.items():\n",
        "    inputs[name] = tensor.to(model.device)\n",
        "\n",
        "# Embed the text\n",
        "with torch.no_grad():\n",
        "    sequence_output, _ = model(**inputs, output_hidden_states=False)\n",
        "\n",
        "# Mean pool the token-level embeddings to get sentence-level embeddings\n",
        "embeddings = torch.sum(\n",
        "    sequence_output * inputs[\"attention_mask\"].unsqueeze(-1), dim=1\n",
        ") / torch.clamp(torch.sum(inputs[\"attention_mask\"], dim=1, keepdims=True), min=1e-9)\n",
        "embeddings = embeddings.cpu()\n",
        "\n",
        "# Compute a semantic similarity via the cosine distance\n",
        "semantic_sim = 1 - cosine(embeddings[0], embeddings[1])\n",
        "print(semantic_sim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QutYGRu9xC5B",
        "colab_type": "text"
      },
      "source": [
        "Currently available models:\n",
        "\n",
        "- [johngiorgi/declutr-small](https://huggingface.co/johngiorgi/declutr-small)\n",
        "- [johngiorgi/declutr-base](https://huggingface.co/johngiorgi/declutr-base)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvpLSGecxC5C",
        "colab_type": "text"
      },
      "source": [
        "## 3Ô∏è‚É£ Bulk embed a file\n",
        "\n",
        "First, lets save our running example to a file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG4kpaJGxC5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = [\n",
        "    \"A smiling costumed woman is holding an umbrella.\",\n",
        "    \"A happy woman in a fairy costume holds an umbrella.\",\n",
        "    \"A soccer game with multiple males playing.\",\n",
        "    \"Some men are playing a sport.\",\n",
        "]\n",
        "text = \"\\n\".join(text)\n",
        "\n",
        "!echo -e \"$text\" > \"input.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOG-sN2SxC5H",
        "colab_type": "text"
      },
      "source": [
        "We then need a pretrained model to embed the text with. Following our running example, lets use DeCLUTR-small"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VoqRh7WxC5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.common.file_utils import cached_path\n",
        "from declutr.encoder import PRETRAINED_MODELS\n",
        "\n",
        "# Download the model OR retrieve its filepath if it has already been downloaded & cached.\n",
        "declutr_small_cached_path = cached_path(PRETRAINED_MODELS[\"declutr-small\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBQfez2axC5O",
        "colab_type": "text"
      },
      "source": [
        "To embed all text in a given file with a trained model, run the following command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wNYDQ7-xC5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# When embedding text with a pretrained model, we do NOT want to sample spans.\n",
        "# We can turn off span sampling by setting the num_anchors attribute to None.\n",
        "overrides = \"{'dataset_reader.num_anchors': null}\"\n",
        "\n",
        "!allennlp predict $declutr_small_cached_path \"input.txt\" \\\n",
        "    --output-file \"embeddings.jsonl\" \\\n",
        "    --batch-size 32 \\\n",
        "    --cuda-device $cuda_device \\\n",
        "    --use-dataset-reader \\\n",
        "    --overrides \"$overrides\" \\\n",
        "    --include-package \"declutr\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u23mh0UxC5W",
        "colab_type": "text"
      },
      "source": [
        "As a sanity check, lets load the embeddings and make sure their cosine similarity is as expected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGYcpU3DxC5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "with open(\"embeddings.jsonl\", \"r\") as f:\n",
        "    embeddings = []\n",
        "    for line in f:\n",
        "        embeddings.append(json.loads(line)[\"embeddings\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrsptLBmxC5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "semantic_sim = 1 - cosine(embeddings[0], embeddings[1])\n",
        "print(semantic_sim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mswpdqCxC5i",
        "colab_type": "text"
      },
      "source": [
        "## ‚ôªÔ∏è Conclusion\n",
        "\n",
        "That's it! In this notebook, we covered three ways to embed text with a pretrained model. Please see [our paper](https://arxiv.org/abs/2006.03659) and [repo](https://github.com/JohnGiorgi/DeCLUTR) for more details, and don't hesitate to open an issue if you have any trouble!"
      ]
    }
  ]
}